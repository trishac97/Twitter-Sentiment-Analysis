{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import tweepy\n",
    "import re\n",
    "import os             \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt')\n",
    "import preprocessor as p\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#input tweeter credentials#\n",
    "CONSUMER_KEY = '3bAsLVoFl8HmlJBwJAB3hATYO'\n",
    "CONSUMER_SECRET = 'mOyV7MHc22fhxGJdUrvVjqHY4cUcsApPBHu0tyiAzenWxCM1wB'\n",
    "OAUTH_TOKEN = '1310696980859899909-1E9G87iXHIUOyPQcHJqxF4nsHptPac'\n",
    "OAUTH_TOKEN_SECRET = 'YPXkDZz2K0dny0NfdS0clfJ4uCCKQVZn0hGaTIR9HV5tN'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "\n",
    "def removeNLTKStop(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_sentences = []\n",
    "    w = []\n",
    "    result_string=\"\"\n",
    "    \n",
    "    text = re.sub(r'\\.COM|[^a-zA-Z ]+|\\s(?=&)|(?<!\\w\\w)\\s+(?!\\w\\w)', '', text, 0, re.IGNORECASE)\n",
    "    \n",
    "    word_tokens = word_tokenize(text.lower()) \n",
    "    print(word_tokens)\n",
    "    \n",
    "    #Removed NLTK Stopwords \n",
    "    filtered_article = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_article = [] \n",
    "    \n",
    "    ps = PorterStemmer() \n",
    "    stemmer = nltk.SnowballStemmer('english')\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    #for s in word_tokens:\n",
    "     #   s = re.sub(r'\\.COM|[^a-zA-Z ]+|\\s(?=&)|(?<!\\w\\w)\\s+(?!\\w\\w)', '', s, 0, re.IGNORECASE)\n",
    "     #   result_string = result_string+s\n",
    "        \n",
    "    print(result_string)\n",
    "    for w in word_tokens: \n",
    "        #w = ps.stem(w)\n",
    "        #w = stemmer.stem(w)\n",
    "        \n",
    "     #   result_string = result_string+s\n",
    "        w = lemmatizer.lemmatize(w,pos=\"v\") \n",
    "        if w not in stop_words: \n",
    "            filtered_article.append(w)\n",
    "    return \" \".join(filtered_article)\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateTweet(tweet):\n",
    "    #translate\n",
    "    translator = Translator()\n",
    "    tweet = translator.translate(tweet) \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet2(tweet):\n",
    "    \n",
    "  \n",
    "    #Remove RT\n",
    "    tweet = re.sub('RT','',tweet)\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.replace('-',' ', 1) \n",
    "    #tweet=re.sub(r'-(?:(?<!\\b[0-9]{4}-)|(?![0-9]{2}(?:[0-9]{2})?\\b))', ' ', tweet)\n",
    "\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Convert @username to ''\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    \n",
    "    tweet = re.sub('u.s.','us',tweet)\n",
    "    tweet = re.sub('united states','us',tweet)\n",
    "\n",
    "\n",
    "    tweet = re.sub('u.k.','uk',tweet)\n",
    "    tweet = re.sub('united kingdom','uk',tweet)\n",
    "\n",
    "\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    print(\"here\",tweet)\n",
    "    #remove NLTK\n",
    "    tweet = removeNLTKStop(tweet)\n",
    "    \n",
    "    \n",
    "    #tweet = tweet.strip('\\'\"')\n",
    "    tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)\n",
    "    tweet.lstrip()\n",
    "\n",
    "\n",
    "    #trim consecutive spaces\n",
    "    tweet = re.sub(' +', ' ', tweet) \n",
    "    \n",
    "    \n",
    "    \n",
    "    tweet = replaceTwoOrMore(tweet)\n",
    "\n",
    "    \n",
    "    # remove emoticons\n",
    "    tweet = p.clean(tweet)\n",
    "    return tweet    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strings =['W','A','R' N I N G']\n",
    "#for s in strings:\n",
    "#    s = re.sub(r'\\.COM|[^a-zA-Z ]+|\\s(?=&)|(?<!\\w\\w)\\s+(?!\\w\\w)', '', s, 0, re.IGNORECASE)\n",
    "#    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "all_files = os.listdir(\"C:\\\\Users\\\\Trisha\\\\Downloads\\\\COVID-19-TweetIDs-master\\\\COVID-19-TweetIDs-master\\\\2020-01\") \n",
    "for a in all_files: \n",
    "    print(\"Starting\",a)\n",
    "    f_name = 'C:\\\\Users\\\\Trisha\\\\Downloads\\\\COVID-19-TweetIDs-master\\\\COVID-19-TweetIDs-master\\\\2020-01\\\\'+a\n",
    "    with open(f_name) as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            line = fp.readline()\n",
    "            #print(line)\n",
    "            try:\n",
    "                api = tweepy.API(auth)\n",
    "                tweet = api.get_status(line,tweet_mode=\"extended\")\n",
    "                if 'retweeted_status' in dir(tweet):\n",
    "                    text=tweet.retweeted_status.full_text\n",
    "                    #print(\"This is retweeted\")\n",
    "                else:\n",
    "                    text=tweet.full_text\n",
    "                    #print(\"This is original tweet\")\n",
    "                #print(tweet.user.location)\n",
    "                #Change language\n",
    "                if(tweet.lang!=\"en\"):\n",
    "                    text=translateTweet(text)\n",
    "                #print(text)\n",
    "                tweet=processTweet2(text)\n",
    "                filename = 'C://Users//Trisha//Tweet_Data//'+str(i)+'.txt'\n",
    "    \n",
    "                f = open(filename, \"w\")\n",
    "\n",
    "                f.write(tweet.lstrip())\n",
    "                f.write(\"\\n\")\n",
    "                i=i+1\n",
    "                f.close()\n",
    "                #print(tweet)\n",
    "            except tweepy.TweepError:\n",
    "                print(\"Cannot fetch tweets\")\n",
    "        \n",
    "        print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
