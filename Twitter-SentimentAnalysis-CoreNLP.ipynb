{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweet-preprocessor in ./.local/lib/python3.5/site-packages (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in ./.local/lib/python3.5/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in ./.local/lib/python3.5/site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.5/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.5/site-packages (from nltk>=3.1->textblob) (0.14.1)\n",
      "Requirement already satisfied: regex in ./.local/lib/python3.5/site-packages (from nltk>=3.1->textblob) (2020.10.28)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.5/site-packages (from nltk>=3.1->textblob) (4.51.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in ./.local/lib/python3.5/site-packages (3.9.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in ./.local/lib/python3.5/site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.5/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tweepy) (1.10.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./.local/lib/python3.5/site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.local/lib/python3.5/site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./.local/lib/python3.5/site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.5/site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in ./.local/lib/python3.5/site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.local/lib/python3.5/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tc2006/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tc2006/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import re\n",
    "import os             \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt')\n",
    "import preprocessor as p\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "#input tweeter credentials#\n",
    "CONSUMER_KEY = '3bAsLVoFl8HmlJBwJAB3hATYO'\n",
    "CONSUMER_SECRET = 'mOyV7MHc22fhxGJdUrvVjqHY4cUcsApPBHu0tyiAzenWxCM1wB'\n",
    "OAUTH_TOKEN = '1310696980859899909-1E9G87iXHIUOyPQcHJqxF4nsHptPac'\n",
    "OAUTH_TOKEN_SECRET = 'YPXkDZz2K0dny0NfdS0clfJ4uCCKQVZn0hGaTIR9HV5tN'\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "\n",
    "def removeNLTKStop(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_sentences = []\n",
    "    w = []\n",
    "    result_string=\"\"\n",
    "    \n",
    "    text = re.sub(r'\\.COM|[^a-zA-Z ]+|\\s(?=&)|(?<!\\w\\w)\\s+(?!\\w\\w)', '', text, 0, re.IGNORECASE)\n",
    "    \n",
    "    word_tokens = word_tokenize(text.lower()) \n",
    "    \n",
    "    #Removed NLTK Stopwords \n",
    "    filtered_article = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_article = [] \n",
    "    \n",
    "    ps = PorterStemmer() \n",
    "    stemmer = nltk.SnowballStemmer('english')\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    #for s in word_tokens:\n",
    "     #   s = re.sub(r'\\.COM|[^a-zA-Z ]+|\\s(?=&)|(?<!\\w\\w)\\s+(?!\\w\\w)', '', s, 0, re.IGNORECASE)\n",
    "     #   result_string = result_string+s\n",
    "        \n",
    "    for w in word_tokens: \n",
    "        #w = ps.stem(w)\n",
    "        #w = stemmer.stem(w)\n",
    "        \n",
    "     #   result_string = result_string+s\n",
    "        w = lemmatizer.lemmatize(w,pos=\"v\") \n",
    "        if w not in stop_words: \n",
    "            filtered_article.append(w)\n",
    "    return \" \".join(filtered_article)\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def translateTweet(tweet):\n",
    "    #translate\n",
    "    translator = Translator()\n",
    "    tweet = translator.translate(tweet) \n",
    "    return tweet\n",
    "\n",
    "def processTweet2(tweet):\n",
    "    \n",
    "  \n",
    "    #Remove RT\n",
    "    tweet = re.sub('RT','',tweet)\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.replace('-',' ', 1) \n",
    "    #tweet=re.sub(r'-(?:(?<!\\b[0-9]{4}-)|(?![0-9]{2}(?:[0-9]{2})?\\b))', ' ', tweet)\n",
    "\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Convert @username to ''\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    \n",
    "    tweet = re.sub('u.s.','us',tweet)\n",
    "    tweet = re.sub('united states','us',tweet)\n",
    "\n",
    "\n",
    "    tweet = re.sub('u.k.','uk',tweet)\n",
    "    tweet = re.sub('united kingdom','uk',tweet)\n",
    "\n",
    "\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #remove NLTK\n",
    "    tweet = removeNLTKStop(tweet)\n",
    "    \n",
    "    \n",
    "    #tweet = tweet.strip('\\'\"')\n",
    "    tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)\n",
    "    tweet.lstrip()\n",
    "\n",
    "\n",
    "    #trim consecutive spaces\n",
    "    tweet = re.sub(' +', ' ', tweet) \n",
    "    \n",
    "    \n",
    "    \n",
    "    tweet = replaceTwoOrMore(tweet)\n",
    "\n",
    "    \n",
    "    # remove emoticons\n",
    "    tweet = p.clean(tweet)\n",
    "    return tweet    \n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_sentiment(tweet): \n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(tweet) \n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tc2006/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Original Tweet======= \n",
      " Also my dad was exposed to covid recently but has thankfully tested negative so maybe I'm catching a break this time\n",
      "=====Processed Tweet======= \n",
      " also dad expose covid recently thankfully test negative maybe im catch break time\n",
      "=====Sentiment Polarity with Naiva Bayes====== \n",
      " negative\n",
      "=====Sentiment Polarity with CoreNLP====== \n",
      "\n",
      "1 (Sentiment Value) Negative (Sentiment)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "tweet = api.get_status(\"1324827582861348866\",tweet_mode=\"extended\")\n",
    "if 'retweeted_status' in dir(tweet):\n",
    "    text=tweet.retweeted_status.full_text\n",
    "                    #print(\"This is retweeted\")\n",
    "else:\n",
    "    text=tweet.full_text\n",
    "print(\"=====Original Tweet======= \\n\",text)\n",
    "tweet=processTweet2(text)\n",
    "print(\"=====Processed Tweet======= \\n\",tweet)\n",
    "\n",
    "results =get_tweet_sentiment(tweet.rstrip())\n",
    "\n",
    "print(\"=====Sentiment Polarity with Naive Bayes====== \\n\",results)\n",
    "\n",
    "\n",
    "print(\"=====Sentiment Polarity with CoreNLP====== \\n\")\n",
    "for s in result[\"sentences\"]:\n",
    "    print(\"{} (Sentiment Value) {} (Sentiment)\".format(\n",
    "   \n",
    "        s[\"sentimentValue\"], s[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pycorenlp in ./.local/lib/python3.5/site-packages (0.3.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.5/site-packages (from pycorenlp) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.5/site-packages (from requests->pycorenlp) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./.local/lib/python3.5/site-packages (from requests->pycorenlp) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./.local/lib/python3.5/site-packages (from requests->pycorenlp) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.local/lib/python3.5/site-packages (from requests->pycorenlp) (1.25.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: nltk[corenlp] in ./.local/lib/python3.5/site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in ./.local/lib/python3.5/site-packages (from nltk[corenlp]) (4.51.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in ./.local/lib/python3.5/site-packages (from nltk[corenlp]) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: regex in ./.local/lib/python3.5/site-packages (from nltk[corenlp]) (2020.10.28)\n",
      "Requirement already satisfied, skipping upgrade: click in ./.local/lib/python3.5/site-packages (from nltk[corenlp]) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: requests; extra == \"corenlp\" in ./.local/lib/python3.5/site-packages (from nltk[corenlp]) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.local/lib/python3.5/site-packages (from requests; extra == \"corenlp\"->nltk[corenlp]) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in ./.local/lib/python3.5/site-packages (from requests; extra == \"corenlp\"->nltk[corenlp]) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in ./.local/lib/python3.5/site-packages (from requests; extra == \"corenlp\"->nltk[corenlp]) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in ./.local/lib/python3.5/site-packages (from requests; extra == \"corenlp\"->nltk[corenlp]) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U nltk[corenlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp.annotate(tweet,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment, ner, pos',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 10000000000000000000000000000000000000000000000000000000000000,\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 'also dad expose covid recently thankfully test negative maybe im catch break time': 1 (Sentiment Value) Negative (Sentiment)\n"
     ]
    }
   ],
   "source": [
    "for s in result[\"sentences\"]:\n",
    "    print(\"{}: '{}': {} (Sentiment Value) {} (Sentiment)\".format(\n",
    "        s[\"index\"],\n",
    "        \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
    "        s[\"sentimentValue\"], s[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': [{'basicDependencies': [{'dep': 'ROOT',\n",
       "     'dependent': 3,\n",
       "     'dependentGloss': 'expose',\n",
       "     'governor': 0,\n",
       "     'governorGloss': 'ROOT'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 1,\n",
       "     'dependentGloss': 'also',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'dep',\n",
       "     'dependent': 2,\n",
       "     'dependentGloss': 'dad',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'dobj',\n",
       "     'dependent': 4,\n",
       "     'dependentGloss': 'covid',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 5,\n",
       "     'dependentGloss': 'recently',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 6,\n",
       "     'dependentGloss': 'thankfully',\n",
       "     'governor': 7,\n",
       "     'governorGloss': 'test'},\n",
       "    {'dep': 'nsubj',\n",
       "     'dependent': 7,\n",
       "     'dependentGloss': 'test',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'},\n",
       "    {'dep': 'amod',\n",
       "     'dependent': 8,\n",
       "     'dependentGloss': 'negative',\n",
       "     'governor': 7,\n",
       "     'governorGloss': 'test'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 9,\n",
       "     'dependentGloss': 'maybe',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'},\n",
       "    {'dep': 'dep',\n",
       "     'dependent': 10,\n",
       "     'dependentGloss': 'im',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'compound',\n",
       "     'dependent': 11,\n",
       "     'dependentGloss': 'catch',\n",
       "     'governor': 13,\n",
       "     'governorGloss': 'time'},\n",
       "    {'dep': 'compound',\n",
       "     'dependent': 12,\n",
       "     'dependentGloss': 'break',\n",
       "     'governor': 13,\n",
       "     'governorGloss': 'time'},\n",
       "    {'dep': 'dobj',\n",
       "     'dependent': 13,\n",
       "     'dependentGloss': 'time',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'}],\n",
       "   'enhancedDependencies': [{'dep': 'ROOT',\n",
       "     'dependent': 3,\n",
       "     'dependentGloss': 'expose',\n",
       "     'governor': 0,\n",
       "     'governorGloss': 'ROOT'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 1,\n",
       "     'dependentGloss': 'also',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'dep',\n",
       "     'dependent': 2,\n",
       "     'dependentGloss': 'dad',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'dobj',\n",
       "     'dependent': 4,\n",
       "     'dependentGloss': 'covid',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 5,\n",
       "     'dependentGloss': 'recently',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 6,\n",
       "     'dependentGloss': 'thankfully',\n",
       "     'governor': 7,\n",
       "     'governorGloss': 'test'},\n",
       "    {'dep': 'nsubj',\n",
       "     'dependent': 7,\n",
       "     'dependentGloss': 'test',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'},\n",
       "    {'dep': 'amod',\n",
       "     'dependent': 8,\n",
       "     'dependentGloss': 'negative',\n",
       "     'governor': 7,\n",
       "     'governorGloss': 'test'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 9,\n",
       "     'dependentGloss': 'maybe',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'},\n",
       "    {'dep': 'dep',\n",
       "     'dependent': 10,\n",
       "     'dependentGloss': 'im',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'compound',\n",
       "     'dependent': 11,\n",
       "     'dependentGloss': 'catch',\n",
       "     'governor': 13,\n",
       "     'governorGloss': 'time'},\n",
       "    {'dep': 'compound',\n",
       "     'dependent': 12,\n",
       "     'dependentGloss': 'break',\n",
       "     'governor': 13,\n",
       "     'governorGloss': 'time'},\n",
       "    {'dep': 'dobj',\n",
       "     'dependent': 13,\n",
       "     'dependentGloss': 'time',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'}],\n",
       "   'enhancedPlusPlusDependencies': [{'dep': 'ROOT',\n",
       "     'dependent': 3,\n",
       "     'dependentGloss': 'expose',\n",
       "     'governor': 0,\n",
       "     'governorGloss': 'ROOT'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 1,\n",
       "     'dependentGloss': 'also',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'dep',\n",
       "     'dependent': 2,\n",
       "     'dependentGloss': 'dad',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'dobj',\n",
       "     'dependent': 4,\n",
       "     'dependentGloss': 'covid',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 5,\n",
       "     'dependentGloss': 'recently',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 6,\n",
       "     'dependentGloss': 'thankfully',\n",
       "     'governor': 7,\n",
       "     'governorGloss': 'test'},\n",
       "    {'dep': 'nsubj',\n",
       "     'dependent': 7,\n",
       "     'dependentGloss': 'test',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'},\n",
       "    {'dep': 'amod',\n",
       "     'dependent': 8,\n",
       "     'dependentGloss': 'negative',\n",
       "     'governor': 7,\n",
       "     'governorGloss': 'test'},\n",
       "    {'dep': 'advmod',\n",
       "     'dependent': 9,\n",
       "     'dependentGloss': 'maybe',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'},\n",
       "    {'dep': 'dep',\n",
       "     'dependent': 10,\n",
       "     'dependentGloss': 'im',\n",
       "     'governor': 3,\n",
       "     'governorGloss': 'expose'},\n",
       "    {'dep': 'compound',\n",
       "     'dependent': 11,\n",
       "     'dependentGloss': 'catch',\n",
       "     'governor': 13,\n",
       "     'governorGloss': 'time'},\n",
       "    {'dep': 'compound',\n",
       "     'dependent': 12,\n",
       "     'dependentGloss': 'break',\n",
       "     'governor': 13,\n",
       "     'governorGloss': 'time'},\n",
       "    {'dep': 'dobj',\n",
       "     'dependent': 13,\n",
       "     'dependentGloss': 'time',\n",
       "     'governor': 10,\n",
       "     'governorGloss': 'im'}],\n",
       "   'entitymentions': [{'characterOffsetBegin': 22,\n",
       "     'characterOffsetEnd': 30,\n",
       "     'docTokenBegin': 4,\n",
       "     'docTokenEnd': 5,\n",
       "     'ner': 'DATE',\n",
       "     'normalizedNER': 'PAST_REF',\n",
       "     'text': 'recently',\n",
       "     'timex': {'tid': 't1', 'type': 'DATE', 'value': 'PAST_REF'},\n",
       "     'tokenBegin': 4,\n",
       "     'tokenEnd': 5}],\n",
       "   'index': 0,\n",
       "   'parse': '(ROOT\\n  (S\\n    (ADVP (RB also))\\n    (VP (NN dad)\\n      (VP (VB expose)\\n        (NP (NN covid))\\n        (ADVP (RB recently))\\n        (SBAR\\n          (S\\n            (NP (RB thankfully) (NN test) (JJ negative))\\n            (VP\\n              (ADVP (RB maybe))\\n              (VBP im)\\n              (NP (NN catch) (NN break) (NN time)))))))))',\n",
       "   'sentiment': 'Negative',\n",
       "   'sentimentDistribution': [0.17063346890168,\n",
       "    0.74035716466205,\n",
       "    0.07830807122607,\n",
       "    0.00615657737681,\n",
       "    0.00454471783339],\n",
       "   'sentimentTree': '(ROOT|sentiment=1|prob=0.740 (ADVP|sentiment=2|prob=0.997 also)\\n  (VP|sentiment=1|prob=0.700 (NN|sentiment=2|prob=0.963 dad)\\n    (VP|sentiment=1|prob=0.663\\n      (@VP|sentiment=2|prob=0.617\\n        (@VP|sentiment=2|prob=0.555 (VB|sentiment=2|prob=0.937 expose) (NP|sentiment=2|prob=0.631 covid))\\n        (ADVP|sentiment=2|prob=0.927 recently))\\n      (SBAR|sentiment=1|prob=0.572\\n        (NP|sentiment=1|prob=0.564 (RB|sentiment=2|prob=0.957 thankfully)\\n          (@NP|sentiment=2|prob=0.453 (NN|sentiment=2|prob=0.986 test) (JJ|sentiment=1|prob=0.485 negative)))\\n        (VP|sentiment=2|prob=0.718 (ADVP|sentiment=2|prob=0.993 maybe)\\n          (@VP|sentiment=2|prob=0.654 (VBP|sentiment=2|prob=0.631 im)\\n            (NP|sentiment=2|prob=0.873 (NN|sentiment=2|prob=0.984 catch)\\n              (@NP|sentiment=2|prob=0.885 (NN|sentiment=2|prob=0.984 break) (NN|sentiment=2|prob=0.999 time)))))))))',\n",
       "   'sentimentValue': '1',\n",
       "   'tokens': [{'after': ' ',\n",
       "     'before': '',\n",
       "     'characterOffsetBegin': 0,\n",
       "     'characterOffsetEnd': 4,\n",
       "     'index': 1,\n",
       "     'lemma': 'also',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'also',\n",
       "     'pos': 'RB',\n",
       "     'word': 'also'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 5,\n",
       "     'characterOffsetEnd': 8,\n",
       "     'index': 2,\n",
       "     'lemma': 'dad',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'dad',\n",
       "     'pos': 'NN',\n",
       "     'word': 'dad'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 9,\n",
       "     'characterOffsetEnd': 15,\n",
       "     'index': 3,\n",
       "     'lemma': 'expose',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'expose',\n",
       "     'pos': 'VB',\n",
       "     'word': 'expose'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 16,\n",
       "     'characterOffsetEnd': 21,\n",
       "     'index': 4,\n",
       "     'lemma': 'covid',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'covid',\n",
       "     'pos': 'NN',\n",
       "     'word': 'covid'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 22,\n",
       "     'characterOffsetEnd': 30,\n",
       "     'index': 5,\n",
       "     'lemma': 'recently',\n",
       "     'ner': 'DATE',\n",
       "     'normalizedNER': 'PAST_REF',\n",
       "     'originalText': 'recently',\n",
       "     'pos': 'RB',\n",
       "     'timex': {'tid': 't1', 'type': 'DATE', 'value': 'PAST_REF'},\n",
       "     'word': 'recently'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 31,\n",
       "     'characterOffsetEnd': 41,\n",
       "     'index': 6,\n",
       "     'lemma': 'thankfully',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'thankfully',\n",
       "     'pos': 'RB',\n",
       "     'word': 'thankfully'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 42,\n",
       "     'characterOffsetEnd': 46,\n",
       "     'index': 7,\n",
       "     'lemma': 'test',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'test',\n",
       "     'pos': 'NN',\n",
       "     'word': 'test'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 47,\n",
       "     'characterOffsetEnd': 55,\n",
       "     'index': 8,\n",
       "     'lemma': 'negative',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'negative',\n",
       "     'pos': 'JJ',\n",
       "     'word': 'negative'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 56,\n",
       "     'characterOffsetEnd': 61,\n",
       "     'index': 9,\n",
       "     'lemma': 'maybe',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'maybe',\n",
       "     'pos': 'RB',\n",
       "     'word': 'maybe'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 62,\n",
       "     'characterOffsetEnd': 64,\n",
       "     'index': 10,\n",
       "     'lemma': 'im',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'im',\n",
       "     'pos': 'VBP',\n",
       "     'word': 'im'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 65,\n",
       "     'characterOffsetEnd': 70,\n",
       "     'index': 11,\n",
       "     'lemma': 'catch',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'catch',\n",
       "     'pos': 'NN',\n",
       "     'word': 'catch'},\n",
       "    {'after': ' ',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 71,\n",
       "     'characterOffsetEnd': 76,\n",
       "     'index': 12,\n",
       "     'lemma': 'break',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'break',\n",
       "     'pos': 'NN',\n",
       "     'word': 'break'},\n",
       "    {'after': '',\n",
       "     'before': ' ',\n",
       "     'characterOffsetBegin': 77,\n",
       "     'characterOffsetEnd': 81,\n",
       "     'index': 13,\n",
       "     'lemma': 'time',\n",
       "     'ner': 'O',\n",
       "     'originalText': 'time',\n",
       "     'pos': 'NN',\n",
       "     'word': 'time'}]}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in result[\"sentences\"]:\n",
    "    print(\"{}: '{}': {} (Sentiment Value) {} (Sentiment)\".format(\n",
    "    s[\"index\"],\n",
    "    \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
    "    s[\"sentimentValue\"], s[\"sentiment\"]))\n",
    "    if(s[\"sentimentValue\"] == \"0\" or s[\"sentimentValue\"] == \"1\" ):\n",
    "        ntweet.append(text)\n",
    "    if(s[\"sentimentValue\"] == \"3\" or s[\"sentimentValue\"] == \"4\" ):\n",
    "        ptweet.append(text)\n",
    "    if (s[\"sentimentValue\"] == \"2\" ):\n",
    "            neutweet.append(text)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
